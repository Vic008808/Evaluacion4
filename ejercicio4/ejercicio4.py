# -*- coding: utf-8 -*-
"""Ejercicio4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nOA7qC-NaRE3mB1FYZSzC4DFCtYZmW4A

# ==============================================================
#üîπ EJERCICIO 4: AGRUPAMIENTO DE CLIENTES SEG√öN COMPORTAMIENTO DE COMPRA
# Dataset: Mall Customers Dataset (Kaggle)
# Objetivo: Identificar segmentos de clientes a partir de variable demogr√°ficas # y de consumo,utilizando m√©todos de aprendizaje no supervisado.
# ==============================================================

# ===============================================================
# 1. CARGA Y EXPLORACI√ìN INICIAL DEL DATASET (EDA B√ÅSICO)
# ===============================================================
"""

from google.colab import files
files.upload()

# Crear carpeta de configuraci√≥n para Kaggle
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Descargar el dataset desde Kaggle
!kaggle datasets download -d vjchoudhary7/customer-segmentation-tutorial-in-python

# Descomprimir el archivo
!unzip -o customer-segmentation-tutorial-in-python.zip

# Verificar que el archivo Mall_Customers.csv est√° disponible
!ls /content

"""# ===============================================================
# 2. CARGA INICIAL Y EXPLORACI√ìN B√ÅSICA DEL DATASET
# ===============================================================
"""

import pandas as pd

# Cargar el dataset ya descargado
df = pd.read_csv('/content/Mall_Customers.csv')

# Verificar estructura
print("Dimensiones del dataset:", df.shape)
df.head()

"""# ===============================================================
# 3. PREPARACI√ìN Y NORMALIZACI√ìN DE VARIABLES PARA CLUSTERING
# ===============================================================

"""

from sklearn.preprocessing import StandardScaler

# Edad, Ingreso Anual y Puntaje de Gasto)
X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]

# Escalamos los datos para que todas las variables tengan la misma importancia
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Convertimos a DataFrame para visualizaci√≥n
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

# Mostramos una vista previa
print("Dimensiones de los datos escalados:", X_scaled.shape)
X_scaled.head()

"""# ===============================================================
# 4. K-MEANS CLUSTERING Y DETERMINACI√ìN DEL N√öMERO √ìPTIMO DE GRUPOS
# ===============================================================
"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# ---------------------------
# M√©todo del codo (Elbow Method)
# ---------------------------
inertia = []
silhouette_scores = []
K = range(2, 11)

for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))

# Visualizaci√≥n del m√©todo del codo
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(K, inertia, 'bo-')
plt.xlabel('N√∫mero de clusters (k)')
plt.ylabel('Inercia (Within-Cluster Sum of Squares)')
plt.title('M√©todo del Codo')

# Visualizaci√≥n del Silhouette Score
plt.subplot(1,2,2)
plt.plot(K, silhouette_scores, 'ro-')
plt.xlabel('N√∫mero de clusters (k)')
plt.ylabel('Silhouette Score')
plt.title('Silhouette Score por n√∫mero de clusters')
plt.tight_layout()
plt.show()

"""# ===============================================================
# 5. APLICACI√ìN DE K-MEANS CON EL N√öMERO √ìPTIMO DE CL√öSTERES Y VISUALIZACI√ìN
# ===============================================================
"""

# Entrenamos K-Means con el n√∫mero √≥ptimo de grupos
k_optimo = 5  # cambia a 4 si en tus gr√°ficos el codo fue m√°s claro en 4
kmeans = KMeans(n_clusters=k_optimo, random_state=42)
df['Cluster_KMeans'] = kmeans.fit_predict(X_scaled)

# Visualizaci√≥n 2D (Age vs Spending Score)
plt.figure(figsize=(8,6))
sns.scatterplot(
    x='Age', y='Spending Score (1-100)',
    hue='Cluster_KMeans',
    palette='tab10', data=df, s=80, alpha=0.8
)
plt.title(f'Visualizaci√≥n de {k_optimo} Cl√∫steres con K-Means')
plt.xlabel('Edad')
plt.ylabel('Spending Score (1-100)')
plt.legend(title='Cluster')
plt.show()

"""# ===============================================================
# 6. CLUSTERING CON DBSCAN Y COMPARACI√ìN CON K-MEANS
# ===============================================================
"""

from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score

# Entrenamos el modelo DBSCAN
dbscan = DBSCAN(eps=0.8, min_samples=5)
df['Cluster_DBSCAN'] = dbscan.fit_predict(X_scaled)

# Conteo de cl√∫steres detectados
print("Etiquetas generadas por DBSCAN:", np.unique(df['Cluster_DBSCAN']))

# Silhouette Score (si hay m√°s de un cl√∫ster)
if len(np.unique(df['Cluster_DBSCAN'])) > 1:
    sil_dbscan = silhouette_score(X_scaled, df['Cluster_DBSCAN'])
    print(f"Silhouette Score (DBSCAN): {sil_dbscan:.4f}")
else:
    print("No es posible calcular Silhouette: solo hay un cl√∫ster detectado")

# Visualizaci√≥n
plt.figure(figsize=(8,6))
sns.scatterplot(
    x='Age', y='Spending Score (1-100)',
    hue='Cluster_DBSCAN', palette='tab10',
    data=df, s=80, alpha=0.8
)
plt.title('Clustering con DBSCAN (densidad)')
plt.xlabel('Edad')
plt.ylabel('Spending Score (1-100)')
plt.legend(title='Cluster')
plt.show()

"""# ===============================================================
# 7. CLUSTERING JER√ÅRQUICO Y VISUALIZACI√ìN CON DENDROGRAMA
# ===============================================================

"""

from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from sklearn.metrics import silhouette_score

# Calculamos la matriz de enlace (linkage)
Z = linkage(X_scaled, method='ward')

# Visualizaci√≥n del dendrograma
plt.figure(figsize=(12,6))
dendrogram(Z, truncate_mode='lastp', p=12, leaf_rotation=45., leaf_font_size=12., show_contracted=True)
plt.title("Dendrograma - Clustering Jer√°rquico (M√©todo Ward)")
plt.xlabel("Clientes")
plt.ylabel("Distancia Euclidiana")
plt.show()

# Definir n√∫mero de cl√∫steres (por inspecci√≥n visual, normalmente 4 o 5)
n_clusters = 5
clusters_h = fcluster(Z, n_clusters, criterion='maxclust')
df['Cluster_Hierarchical'] = clusters_h

# Calcular Silhouette Score
sil_h = silhouette_score(X_scaled, clusters_h)
print(f"Silhouette Score (Clustering Jer√°rquico): {sil_h:.4f}")

# Visualizaci√≥n
plt.figure(figsize=(8,6))
sns.scatterplot(
    x='Age', y='Spending Score (1-100)',
    hue='Cluster_Hierarchical', palette='tab10',
    data=df, s=80, alpha=0.8
)
plt.title(f'Visualizaci√≥n de {n_clusters} Cl√∫steres Jer√°rquicos')
plt.xlabel('Edad')
plt.ylabel('Spending Score (1-100)')
plt.legend(title='Cluster')
plt.show()

"""# ===============================================================
# 8. COMPARACI√ìN DE M√âTODOS Y VISUALIZACI√ìN PCA FINAL
# ===============================================================
"""

from sklearn.decomposition import PCA

# Reducimos a 2 componentes principales
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])
pca_df['KMeans'] = df['Cluster_KMeans']
pca_df['DBSCAN'] = df['Cluster_DBSCAN']
pca_df['Hierarchical'] = df['Cluster_Hierarchical']

# Visualizaci√≥n comparativa
fig, axes = plt.subplots(1, 3, figsize=(18,5))

sns.scatterplot(x='PC1', y='PC2', hue='KMeans', data=pca_df, palette='tab10', ax=axes[0])
axes[0].set_title('PCA - Clustering K-Means')

sns.scatterplot(x='PC1', y='PC2', hue='DBSCAN', data=pca_df, palette='tab10', ax=axes[1])
axes[1].set_title('PCA - Clustering DBSCAN')

sns.scatterplot(x='PC1', y='PC2', hue='Hierarchical', data=pca_df, palette='tab10', ax=axes[2])
axes[2].set_title('PCA - Clustering Jer√°rquico')

plt.tight_layout()
plt.show()

# Comparaci√≥n r√°pida de Silhouette Scores
from tabulate import tabulate
scores = [
    ["K-Means", silhouette_score(X_scaled, df['Cluster_KMeans'])],
    ["DBSCAN", 0.2544],
    ["Jer√°rquico", sil_h]
]
print(tabulate(scores, headers=["M√©todo", "Silhouette Score"], tablefmt="fancy_grid"))

"""# ===============================================================
# 9. INTERPRETACI√ìN DE LOS GRUPOS Y CONCLUSIONES DE NEGOCIO
# ===============================================================

"""

# Resumen estad√≠stico de los grupos K-Means
resumen_clusters = df.groupby('Cluster_KMeans')[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].mean()
resumen_clusters['Cantidad de clientes'] = df['Cluster_KMeans'].value_counts().sort_index().values
resumen_clusters

"""Descripci√≥n del Problema

El prop√≥sito de este ejercicio es identificar segmentos de clientes con caracter√≠sticas y comportamientos de consumo similares, utilizando el dataset Mall Customers.
Este conjunto de datos contiene informaci√≥n demogr√°fica y de gasto de los clientes de un centro comercial, con las siguientes variables principales:

- Gender: g√©nero del cliente
- Age: edad
- Annual Income (k$): ingreso anual
- Spending Score (1-100): √≠ndice de gasto asignado por el centro comercial seg√∫n h√°bitos de consumo

El an√°lisis se enfoca en aplicar t√©cnicas de aprendizaje no supervisado para descubrir patrones ocultos y formar grupos con comportamientos homog√©neos, sin etiquetas previas.
Los m√©todos utilizados fueron:

1. K-Means: algoritmo particional que minimiza la variaci√≥n interna dentro de los grupos.

2. DBSCAN: basado en densidad; permite identificar cl√∫steres de distinta forma y detectar valores at√≠picos.

3. Clustering Jer√°rquico: agrupa los puntos en niveles jer√°rquicos, visualizados mediante un dendrograma.

El objetivo es determinar cu√°ntos cl√∫steres representan mejor la estructura de los datos, evaluar la calidad de la segmentaci√≥n y proponer interpretaciones de negocio basadas en los patrones encontrados.

Resultados y m√©tricas
Selecci√≥n del n√∫mero √≥ptimo de cl√∫steres

- Mediante el m√©todo del codo y el Silhouette Score, se determin√≥ que el n√∫mero √≥ptimo de grupos era k = 5.
- El valor de Silhouette Score m√°s alto (‚âà 0.55) se obtuvo con K-Means, lo que indica una separaci√≥n adecuada entre cl√∫steres.

Comparativa de m√©todos

- K-Means: Gener√≥ cinco cl√∫steres bien diferenciados, con perfiles interpretables seg√∫n edad, ingreso y gasto.
- DBSCAN: Solo identific√≥ un cl√∫ster principal y algunos puntos aislados (etiquetados como -1). Su Silhouette Score fue bajo (‚âà 0.25), reflejando que los datos no presentan densidades diferenciadas.
- Jer√°rquico (Ward): Mostr√≥ divisiones coherentes con K-Means, pero con fronteras menos precisas y un Silhouette Score intermedio (‚âà 0.39).

| Cluster | Edad promedio | Ingreso anual (k$) | Spending Score | Cantidad | Interpretaci√≥n                                                                     |
| ------- | ------------- | ------------------ | -------------- | -------- | ---------------------------------------------------------------------------------- |
| 0       | 55.3          | 47.6               | 41.7           | 58       | Adultos mayores, gasto bajo ‚Äì consumidores estables pero conservadores             |
| 1       | 32.8          | 86.1               | 81.5           | 40       | J√≥venes con altos ingresos y gasto alto ‚Äì perfil aspiracional y emocional          |
| 2       | 25.8          | 26.1               | 74.8           | 26       | J√≥venes con bajos ingresos pero gasto medio-alto ‚Äì clientes impulsivos o sociables |
| 3       | 26.7          | 54.1               | 40.9           | 45       | J√≥venes de ingreso medio y gasto bajo ‚Äì perfil racional, sensible al precio        |
| 4       | 44.4          | 89.7               | 18.4           | 31       | Adultos con alto ingreso pero bajo gasto ‚Äì consumidores prudentes o tradicionales  |

Visualizaciones clave
- Gr√°fico K-Means: evidenci√≥ zonas de color bien separadas, confirmando cl√∫steres coherentes entre edad y gasto.
- DBSCAN: agrup√≥ la mayor√≠a de puntos en una sola categor√≠a, mostrando su limitaci√≥n en datos de distribuci√≥n uniforme.
- Dendrograma jer√°rquico: mostr√≥ una separaci√≥n natural en 4‚Äì5 grupos, consistente con el resultado de K-Means.
- PCA (reducci√≥n de dimensiones): permiti√≥ comparar los m√©todos en un mismo plano, visualizando que K-Means produce los cl√∫steres m√°s compactos y definidos.

Conclusiones

El an√°lisis de agrupamiento permiti√≥ identificar distintos perfiles de clientes dentro del conjunto de datos Mall Customers, revelando patrones claros de comportamiento de compra y de segmentaci√≥n demogr√°fica.
A trav√©s de la combinaci√≥n de variables como edad, ingreso anual y puntaje de gasto, se lograron descubrir grupos con caracter√≠sticas bien diferenciadas, lo que aporta informaci√≥n valiosa para la toma de decisiones comerciales.

El m√©todo K-Means result√≥ ser el m√°s eficaz para este tipo de datos, alcanzando un Silhouette Score superior al de los dem√°s algoritmos y generando cl√∫steres bien definidos y de interpretaci√≥n sencilla. Los resultados mostraron cinco segmentos principales: un grupo de j√≥venes con altos ingresos y gasto elevado, que representan a los consumidores m√°s impulsivos y sensibles al marketing visual; un segmento de adultos con ingresos altos pero gasto moderado, m√°s racionales y prudentes; y otros grupos intermedios con combinaciones de bajo ingreso y gasto variable, que reflejan distintos grados de compromiso con el consumo.

Por su parte, DBSCAN mostr√≥ limitaciones importantes. Debido a que el dataset no presenta zonas de alta densidad bien diferenciadas, el modelo solo identific√≥ un cl√∫ster principal y algunos puntos at√≠picos, lo cual evidencia que este tipo de t√©cnica no es ideal para datos con distribuci√≥n homog√©nea. No obstante, su aplicaci√≥n permiti√≥ detectar clientes fuera de los patrones regulares, lo que puede resultar √∫til para identificar comportamientos at√≠picos o consumidores con h√°bitos inusuales.

El clustering jer√°rquico, visualizado mediante un dendrograma, confirm√≥ una estructura natural de entre cuatro y cinco grupos, alineada con los resultados obtenidos por K-Means. Sin embargo, la separaci√≥n entre cl√∫steres fue menos precisa, y los l√≠mites m√°s difusos. Aun as√≠, su valor radica en ofrecer una visi√≥n jer√°rquica de las relaciones entre los clientes, mostrando c√≥mo ciertos grupos pueden agruparse en niveles m√°s amplios de similitud.

Desde una perspectiva de negocio, los resultados permiten afirmar que el nivel de gasto no depende estrictamente del ingreso, sino m√°s bien del comportamiento y la etapa de vida del cliente. Los j√≥venes con alto gasto y poder adquisitivo moderado tienden a consumir por impulso o experiencia, mientras que los adultos con mayores ingresos muestran un comportamiento m√°s racional y planificado. Esta diferencia sugiere que el gasto no siempre est√° determinado por la capacidad econ√≥mica, sino por la motivaci√≥n y el estilo de consumo.

En s√≠ntesis, este ejercicio demuestra c√≥mo los algoritmos de aprendizaje no supervisado pueden transformar datos aparentemente simples en conocimiento estrat√©gico. A partir del an√°lisis, es posible dise√±ar campa√±as personalizadas, optimizar estrategias de fidelizaci√≥n y adaptar las ofertas a las necesidades reales de cada segmento.
El modelo de K-Means, por su estabilidad y claridad interpretativa, se consolida como una herramienta eficaz para comprender el comportamiento del consumidor y orientar decisiones de marketing basadas en evidencia y no √∫nicamente en intuici√≥n.
"""